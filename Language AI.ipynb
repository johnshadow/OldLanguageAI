{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a79ae81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8defd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "print(get_available_devices()) \n",
    "print(tf.config.list_physical_devices())\n",
    "path_to_file = \"A:/Neural Network/IliadExodus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e41af64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 836287 characters and 135435 words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters and {text.count(\" \") + text.count(chr(13))} words')\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97c7c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make some basic functions, so tensor flow can read our texts\n",
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d37c9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make different small sequences\n",
    "seq_length = 100\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ed39c26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#chop up text into small bits for easier training.\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "103723b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset in chuncks acceptable by tensorflow\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "split_input_target(list(\"Tensorflow\"))\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "917922d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8942974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2f680b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, inputs):\n",
    "        inputs, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(inputs, training=True)\n",
    "            loss = self.loss(labels, predictions)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4dc6a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8d4573b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 223) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3fcd6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     multiple                  57088     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 multiple                  3938304   \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  228575    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,223,967\n",
      "Trainable params: 4,223,967\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "468b89c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 223)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(5.4077315, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "096508da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1cf50c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "129/129 [==============================] - 19s 132ms/step - loss: 3.4659\n",
      "Epoch 2/20\n",
      "129/129 [==============================] - 19s 132ms/step - loss: 2.4391\n",
      "Epoch 3/20\n",
      "129/129 [==============================] - 20s 140ms/step - loss: 2.1713\n",
      "Epoch 4/20\n",
      "129/129 [==============================] - 19s 135ms/step - loss: 1.9832\n",
      "Epoch 5/20\n",
      "129/129 [==============================] - 19s 138ms/step - loss: 1.8012\n",
      "Epoch 6/20\n",
      "129/129 [==============================] - 19s 137ms/step - loss: 1.6458\n",
      "Epoch 7/20\n",
      "129/129 [==============================] - 18s 136ms/step - loss: 1.5219\n",
      "Epoch 8/20\n",
      "129/129 [==============================] - 19s 139ms/step - loss: 1.4175\n",
      "Epoch 9/20\n",
      "129/129 [==============================] - 18s 134ms/step - loss: 1.3271\n",
      "Epoch 10/20\n",
      "129/129 [==============================] - 18s 134ms/step - loss: 1.2447\n",
      "Epoch 11/20\n",
      "129/129 [==============================] - 18s 134ms/step - loss: 1.1700\n",
      "Epoch 12/20\n",
      "129/129 [==============================] - 18s 135ms/step - loss: 1.0983\n",
      "Epoch 13/20\n",
      "129/129 [==============================] - 19s 140ms/step - loss: 1.0286\n",
      "Epoch 14/20\n",
      "129/129 [==============================] - 19s 140ms/step - loss: 0.9609\n",
      "Epoch 15/20\n",
      "129/129 [==============================] - 20s 145ms/step - loss: 0.8898\n",
      "Epoch 16/20\n",
      "129/129 [==============================] - 19s 134ms/step - loss: 0.8237\n",
      "Epoch 17/20\n",
      "129/129 [==============================] - 20s 139ms/step - loss: 0.7549\n",
      "Epoch 18/20\n",
      "129/129 [==============================] - 19s 138ms/step - loss: 0.6888\n",
      "Epoch 19/20\n",
      "129/129 [==============================] - 19s 140ms/step - loss: 0.6261\n",
      "Epoch 20/20\n",
      "129/129 [==============================] - 18s 131ms/step - loss: 0.5679\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77532a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bfeb37f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "οἰ δε Ἀχιλῆοι καὶ τε Ἑβραίων παρὰ τοῦ ἑνὸς αὐτῆς, καὶ ἔσται Ἀχιλῆα προχείπονται ὁ ταῦρος καὶ ὅ κε σὺ καὶ πάχας ἐπὶ δὲ τῆς κεφαλῇ, καὶ ἐπίστευσαν ὥστα ἐπὶ τῶν στύλων ἡμῶν, καὶ εἶπε Μωυσῆς·\n",
      "τί λοπτοῦ·\n",
      "Τὸρ δμῳθήτηρ Κύριος ἡμέρᾳ τῇ ἑβδόμῃ·\n",
      "καὶ ἰδοὺ οὐκ ἐξ ὑπὸ τὴν ἁμὸν κατὰ καλίν.\r\n",
      "καθότι οὐδὲ ἔτι μᾶλλον ὅτι ἐπὶ τῆς γῆς, καὶ προσήγαγον αὐτοὺς ὑπὸ τὸ ὕδωρ, καὶ ἔσται, ὅτι μὴ ραὸς τῆς Φαραὼ καὶ Ἀλυονῆφαι Φαραώ·\n",
      "τοῦτό μόματος τρόμους ἔκουσον πυρὰ τοῦ Θεοῦ καὶ ἀποθάνῃ, λαβλητὰ τὴν παρθνόν σου καὶ ἡμίσους τὸ ὕψος·\n",
      "αὐτὸν δὲ οἱ ἄρχοντες τοῖς ζεοῖς.\r\n",
      "καὶ ποιήσεις τὰ ἔργα Ἰσραὴλ τῷ στύλῳ; τὰ δὲ βραχύοντες ἔνυσαν ἀπέλῃκε, καὶ τρεῖς κρατῆρ αὐτῆς καὶ λίθους τοῦ Ἰσραήλ, πᾶς εἰσακάριες αὐτήν.\r\n",
      "καὶ πεντήκοντα πήχεων τὸ μέσον τοῦ ἐνιαυτοῦ λαῷ·\n",
      "ἐν γὰρ χειρὶ κρατύσω ὑμᾶς καὶ πάσῃς τῆς φωνῆς μί τις πρὸς ὄψομαι, εἰς ὄφρα πάντα τὸν λαόν μου τὰ ἔργεν ὑμῖν αὐτῷ·\n",
      "τά γὰρ καθάπερ χθές, καὶ οὐ δυνήσονται ὅτι νότος καὶ πάντες οἱ Αἰγύπτιοι Μωυσῆς μέγα τῆς θῆκεν ὑμῖν μαχαῖρας ἐπὶ σέ, γούνυκτο δὲ ταῦτα μαχήσεται αὐτὰ καὶ υἱοὶ Ἰσραὴλ τοὺς ἀναίτιος, ἄ \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 5.615875959396362\n"
     ]
    }
   ],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['οἰ δε Ἀχιλῆοι καὶ τε Ἑβραίων'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512eb28d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
